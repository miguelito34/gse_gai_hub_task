{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSE GenAI Research Repo Web Scraper\n",
    "\n",
    "**Author:** Michael Spencer\n",
    "\n",
    "**Purpose:** Serves as an interactive environment to write and troubleshoot a python script for scraping text data from the Generative AI for Education Hub Research Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Objective:** To evaluate your ability to effectively scrape, process, and analyze text data from the web,\n",
    "focusing on the provided data sources.\n",
    "\n",
    "**Task Overview:** You are tasked with scraping text data from the Generative AI for Education Hub -\n",
    "specifically the [Research Repository](https://scale.stanford.edu/genai/repository?search_api_fulltext=&application%5B42%5D=42&benefits%5B34%5D=34&benefits%5B36%5D=36&benefits%5B35%5D=35).\n",
    "\n",
    "**Expected Working Time:** 2 hours or less\n",
    "\n",
    "Step-by-Step Instructions:\n",
    "1. Data Scraping\n",
    "- Write a Python script (or use a tool of your choice) to scrape the above web pages.\n",
    "- Focus on extracting key text elements relevant to Teaching - Instructional Materials in\n",
    "K12 and Impact - Randomized Controlled Trials in secondary education.\n",
    "- Save the scraped data in a structured format (e.g., CSV, JSON).\n",
    "2. Basic Text Analysis\n",
    "- Perform a basic keyword frequency analysis on the scraped data.\n",
    "- Provide a brief summary (2-3 sentences) of the key information extracted from the data.\n",
    "- Present your findings in a concise report (max 1 page, tables/figures excluded) that\n",
    "includes a list of all papers’ metadata (title, author(s), date, etc.).\n",
    "3. Documentation and Submission\n",
    "- Document your process, including the tools and methods used (not to exceed ½ page).\n",
    "- Submit the following:\n",
    "  - The Python script (or other tool-based approach) used for scraping.\n",
    "  - The scraped data file (CSV/JSON).\n",
    "  - The analysis report.\n",
    "\n",
    "Evaluation Criteria\n",
    "- Technical Proficiency: Effectiveness and efficiency of the scraping script, choice of tools, and\n",
    "handling of data.\n",
    "- Data Quality: Relevance, completeness, and structure of the scraped data.\n",
    "- Analytical Insight: Quality and relevance of the text analysis and the summary report.\n",
    "- Documentation: Clarity and thoroughness of the process documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checked robots.txt on site to see if there were any scraping limits. There didn't seem to be so I went ahead and scraped the site, only to get blocked. To combat this, I added a half second delay to my requests.\n",
    "- There are three articles that belong to both searches. In total as of 2025-02-02, there were 77 articles.\n",
    "- Each research article is nested within the `<li class=\"col\">` tag so I can use those to identify articles to parse.\n",
    "- The url for each article's individual page is the first href in the above element. I could use that to scrape the abstracts.\n",
    "- I will scrape the data first, save it, and then perform an analysis. This separation allows for more modular code, that is easier to maintain. I am also not dealing with a substantial amount of data, and hence more advanced streaming processing methods are not neccessarily needed here.\n",
    "- I will utilize the repos search API feature to search for only topics that I are interested in per the task. This makes sense given the timed nature of this task.\n",
    "\n",
    "Potential Improvements:\n",
    "- Implement parellel requests\n",
    "- Combine the page gathering step and the article scraping step to minimize repeated requests and reduce code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install requests beautifulsoup4 pandas pathlib os time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment libraries\n",
    "from pathlib import Path\n",
    "\n",
    "# Analysis libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = str(Path.cwd().resolve().parent)\n",
    "DATA_DIR = PROJECT_ROOT + \"/data\"\n",
    "DATA_OUT = DATA_DIR + \"/clean/test_gse_genai_articles.csv\"\n",
    "\n",
    "# Sets up the URLS for our HTML requests. Opting to use hard coded search API URLs to save time, and so\n",
    "# that we don't have to do additional parsing to gather the relevant files.\n",
    "REPO_BASE_URL = \"https://scale.stanford.edu\"\n",
    "TEACHING_K12_SEARCH_URL = REPO_BASE_URL + \"/genai/repository?search_api_fulltext=&application%5B42%5D=42&benefits%5B34%5D=34&benefits%5B36%5D=36&benefits%5B35%5D=35\"\n",
    "# \"Secondary\" is defined as middle and high school here.\n",
    "IMPACT_SECONDARY_SEARCH_URL = REPO_BASE_URL + \"/genai/repository?search_api_fulltext=&benefits%5B36%5D=36&benefits%5B35%5D=35&study_design%5B55%5D=55\"\n",
    "SEARCH_URLS = [TEACHING_K12_SEARCH_URL, IMPACT_SECONDARY_SEARCH_URL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(search_urls):\n",
    "    page_urls = identify_pages_to_scrape(search_urls)\n",
    "    article_urls = identify_articles_to_scrape(page_urls)\n",
    "    article_data = extract_article_data(article_urls)\n",
    "    write_data_to_csv(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes the search results for every \"Teaching - Instructional Materials in K12\" and\n",
    "# \"Impact - Randomized Controlled Trials in secondary education\" page,\n",
    "# and adds them to the set of pages in which to look for articles.\n",
    "def identify_pages_to_scrape(search_urls):\n",
    "    pages_to_scrape = set()\n",
    "\n",
    "    for search_url in search_urls:\n",
    "        response = requests.get(search_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Warning: Failed to retrieve {search_url} (status: {response.status_code}).\")\n",
    "            continue\n",
    "        \n",
    "        parsed_response = BeautifulSoup(response.text, 'html.parser')\n",
    "        pagination_data = parsed_response.select(\"ul.pagination li a.page-link\")\n",
    "\n",
    "        # Catches the case where there is no pagination data, and we only have one page to scrape.\n",
    "        if not pagination_data:\n",
    "            pages_to_scrape.add(search_url)\n",
    "            continue\n",
    "        \n",
    "        for page in pagination_data:\n",
    "            page_url = REPO_BASE_URL + \"/genai/repository\" + page['href']\n",
    "            pages_to_scrape.add(page_url)\n",
    "\n",
    "    return pages_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes each previously identified page for the article urls and\n",
    "# adds them to the set of articles to scrape.\n",
    "def identify_articles_to_scrape(page_urls):\n",
    "    articles_to_scrape = set()\n",
    "    article_titles = set()\n",
    "\n",
    "    for page_url in page_urls:\n",
    "        response = requests.get(page_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Warning: Failed to retrieve {page_url} (status: {response.status_code}).\")\n",
    "            continue\n",
    "        \n",
    "        parsed_response = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles_to_parse = parsed_response.select(\"ul.list-papers li.col\")\n",
    "        \n",
    "        # Extracts the individual article URLs from the page HTML.\n",
    "        for article in articles_to_parse:\n",
    "            article_sub_url = article.select_one(\"div.card a[href]\")\n",
    "            article_url = REPO_BASE_URL + article_sub_url['href']\n",
    "\n",
    "            # Checks if the article title is already in the set of article titles to avoid duplicates.\n",
    "            article_title = article.select_one(\"div.card a[hreflang='en']\").get_text(strip=True)\n",
    "            if article_title not in article_titles:\n",
    "                article_titles.add(article_title)\n",
    "                articles_to_scrape.add(article_url)\n",
    "\n",
    "    print(f\"Identified {len(articles_to_scrape)} distinct articles to scrape.\")\n",
    "\n",
    "    return articles_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses metadata for analysis from each article\n",
    "def extract_article_data(articles_to_scrape):\n",
    "    article_data = []\n",
    "\n",
    "    print(f\"Attempting to scrape data from {len(articles_to_scrape)} articles...\")\n",
    "\n",
    "    for article in articles_to_scrape:\n",
    "        response = requests.get(article)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Warning: Failed to retrieve {article} (status: {response.status_code}).\")\n",
    "            continue\n",
    "\n",
    "        parsed_article = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Gathers metadata from the article\n",
    "        article_metadata = {}\n",
    "        title = parsed_article.select_one(\"h1\").get_text(strip=True)\n",
    "        article_metadata[\"Title\"] = title\n",
    "\n",
    "        # Identifies all the metadata fields within the article HTML node.\n",
    "        node_content = parsed_article.select_one(\"div.node__content\").select(\"div.field\")\n",
    "\n",
    "        for field in node_content:\n",
    "            article_metadata = extract_metadata_field(field, article_metadata)\n",
    "\n",
    "        article_data.append(article_metadata)\n",
    "\n",
    "    print(f\"Successfully scraped data from {len(article_data)} distinct articles.\")\n",
    "\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses each field of the article and returns an updated dictionary of the relevant metadata.\n",
    "def extract_metadata_field(metadata_field, article_metadata):\n",
    "    field_name_html = metadata_field.select_one(\"div.field__label\")\n",
    "        \n",
    "    # If the field name is not found, I assume the field is the \"Abstract\" based on the site structure.\n",
    "    if not field_name_html:\n",
    "        field_name = \"Abstract\"\n",
    "        field_value = metadata_field.select_one(\"div.field__item, p\").get_text(strip=True)\n",
    "        article_metadata[field_name] = field_value\n",
    "        return article_metadata\n",
    "    \n",
    "    # If the field name is found, I use the label given and extract the relevant items, however many there may be.\n",
    "    field_name = field_name_html.get_text(strip=True)\n",
    "    field_items = metadata_field.select(\"div.field__item\")\n",
    "    \n",
    "    item_values = \"\"\n",
    "    # If the field name is \"Link\", extract the href attribute\n",
    "    if field_name == \"Link\":\n",
    "        publishing_link = metadata_field.select_one(\"a[href]\")\n",
    "        field_items = publishing_link[\"href\"]\n",
    "    else:\n",
    "        for item in field_items:\n",
    "            # Some fields have multiple items, so I concatenate them together and strip unneccessary linebreaks\n",
    "            item_value = item.get_text().strip(\"\\n\")\n",
    "            item_values = item_values + item_value\n",
    "\n",
    "    # Add the field to the dictionary.\n",
    "    article_metadata[field_name] = item_values\n",
    "\n",
    "    return article_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the scraped data to a CSV\n",
    "def write_data_to_csv(data):\n",
    "    data_gse_genai_articles = pd.DataFrame(data)\n",
    "\n",
    "    # Rename the \"Who Age?\" column to \"What Age?\" and save to a CSV file.\n",
    "    (data_gse_genai_articles.rename(columns={\"Who age?\": \"What age?\"})\n",
    "                            .sort_values(by=[\"Title\"])\n",
    "                            .to_csv(DATA_DIR + \"/clean/gse_genai_articles.csv\", index=False))\n",
    "    \n",
    "    print(f\"Wrote data to CSV at data {DATA_OUT}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 75 distinct articles to scrape.\n",
      "Attempting to scrape data from 75 articles...\n",
      "Successfully scraped data from 75 distinct articles.\n",
      "Wrote data to CSV at data /Users/michaelspencer/projects/interviews/gse_gai_hub_task/part1_webscraping/data/clean/test_gse_genai_articles.csv.\n"
     ]
    }
   ],
   "source": [
    "main(search_urls = SEARCH_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Plan\n",
    "- Gather the full URLs that I must search for based on how many pages each of the two topic searches yield\n",
    "- For each URL/page:\n",
    "  - Make a request\n",
    "  - Gather article URLs to also include abstracts. The metadata also appears easier to iterate through on the individual articles pages.\n",
    "  - Parse HTML to store the metadata for each article in dictionaries. Be wary of duplicate articles:\n",
    "    - Title\n",
    "    - Author(s)\n",
    "    - Application(s)\n",
    "    - Age(s)\n",
    "    - Uses\n",
    "    - Study Design\n",
    "- Save the data to a CSV or JSON file\n",
    "- Read in data file\n",
    "- Conduct basis frequency analysis of words\n",
    "   - Clean titles by lowercasing, removing stop words, lemmatizing, etc.\n",
    "- Report on key findings\n",
    "- Create report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes the search results for every \"Teaching - Instructional Materials in K12\" and \"Impact - Randomized Controlled Trials\n",
    "# in secondary education\" page, and adds them to the set of pages in which to look for articles.\n",
    "PAGES_TO_SCRAPE = set()\n",
    "\n",
    "for search_url in [TEACHING_K12_SEARCH_URL, IMPACT_SECONDARY_SEARCH_URL]:\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Warning: Failed to retrieve {search_url} (status: {response.status_code}).\")\n",
    "        continue\n",
    "    \n",
    "    parsed_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    pagination_data = parsed_response.select(\"ul.pagination li a.page-link\")\n",
    "\n",
    "    # Catches the case where there is no pagination data, and we only have one page to scrape.\n",
    "    if not pagination_data:\n",
    "        PAGES_TO_SCRAPE.add(search_url)\n",
    "        continue\n",
    "    \n",
    "    for page in pagination_data:\n",
    "        page_url = REPO_BASE_URL + \"/genai/repository\" + page['href']\n",
    "        PAGES_TO_SCRAPE.add(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 77 distinct articles to scrape.\n"
     ]
    }
   ],
   "source": [
    "# Scrapes each identified page for the article urls and adds them to the set of articles to scrape.\n",
    "ARTICLES_TO_SCRAPE = set()\n",
    "\n",
    "for page_url in PAGES_TO_SCRAPE:\n",
    "    response = requests.get(page_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Warning: Failed to retrieve {page_url} (status: {response.status_code}).\")\n",
    "        continue\n",
    "    \n",
    "    parsed_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles_to_parse = parsed_response.select(\"ul.list-papers li.col\")\n",
    "    \n",
    "    for article in articles_to_parse:\n",
    "        article_sub_url = article.select_one(\"div.card a[href]\")\n",
    "        article_url = REPO_BASE_URL + article_sub_url['href']\n",
    "        ARTICLES_TO_SCRAPE.add(article_url)\n",
    "\n",
    "print(f\"Identified {len(ARTICLES_TO_SCRAPE)} distinct articles to scrape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape data from 77 articles...\n",
      "Successfully scraped data from 77 distinct articles.\n"
     ]
    }
   ],
   "source": [
    "# Parses relevant data from each article\n",
    "ARTICLE_DATA = []\n",
    "\n",
    "print(f\"Attempting to scrape data from {len(ARTICLES_TO_SCRAPE)} articles...\")\n",
    "\n",
    "for article in ARTICLES_TO_SCRAPE:\n",
    "    response = requests.get(article)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Warning: Failed to retrieve {article} (status: {response.status_code}).\")\n",
    "        continue\n",
    "\n",
    "    parsed_article = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Gathers metadata from the article\n",
    "    article_metadata = {}\n",
    "    title = parsed_article.select_one(\"h1\").get_text(strip=True)\n",
    "    article_metadata[\"Title\"] = title\n",
    "\n",
    "    node_content = parsed_article.select_one(\"div.node__content\").select(\"div.field\")\n",
    "    for field in node_content:\n",
    "        field_name_html = field.select_one(\"div.field__label\")\n",
    "        \n",
    "        # If the field name is not found, I assume the field is the \"Abstract\" based on the site structure.\n",
    "        if not field_name_html:\n",
    "            field_name = \"Abstract\"\n",
    "            field_value = field.select_one(\"div.field__item, p\").get_text(strip=True)\n",
    "            article_metadata[field_name] = field_value\n",
    "            continue\n",
    "        \n",
    "        # If the field name is found, I use the label given and extract the relevant items, however many there may be.\n",
    "        field_name = field_name_html.get_text(strip=True)\n",
    "        field_items = field.select(\"div.field__item\")\n",
    "\n",
    "        item_values = \"\"\n",
    "        # If the field name is \"Link\", extract the href attribute\n",
    "        if field_name == \"Link\":\n",
    "            publishing_link = field.select_one(\"a[href]\")\n",
    "            item_values = publishing_link[\"href\"]\n",
    "        else:\n",
    "            for item in field_items:\n",
    "                # Some fields have multiple items, so I concatenate them together and strip unneccessary linebreaks\n",
    "                item_value = item.get_text().strip(\"\\n\")\n",
    "                item_values = item_values + item_value\n",
    "\n",
    "        article_metadata[field_name] = item_values\n",
    "\n",
    "    ARTICLE_DATA.append(article_metadata)\n",
    "\n",
    "print(f\"Successfully scraped data from {len(ARTICLE_DATA)} distinct articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gse_genai_articles = pd.DataFrame(ARTICLE_DATA)\n",
    "\n",
    "# # Rename the \"Who Age?\" column to \"What Age?\" and save to a CSV file.\n",
    "# (data_gse_genai_articles.rename(columns={\"Who age?\": \"What age?\"})\n",
    "#                         .sort_values(by=[\"Title\"])\n",
    "#                         .to_csv(DATA_DIR + \"/clean/gse_genai_articles.csv\", index=False))\n",
    "\n",
    "data_gse_genai_articles.rename(columns={\"Who age?\": \"What age?\"}).sort_values(by=[\"Title\"]).to_csv(DATA_DIR + \"/clean/gse_genai_articles.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
